{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36980cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3a5f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'D:/datasets/RHD_v1-1/RHD_published_v2/training/depth/00001.png'\n",
    "io = imageio.imread(dir)\n",
    "cv = cv2.imread(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acddea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(io[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa643ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833709e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = cv2.cvtColor(cv, cv2.COLOR_BGR2RGB)\n",
    "print(t[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1061e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_two_uint8_to_float(top_bits, bottom_bits):\n",
    "    \"\"\" Converts a RGB-coded depth into float valued depth. \"\"\"\n",
    "    depth_map = (top_bits * 2**8 + bottom_bits).astype('float32')\n",
    "    depth_map /= float(2**16 - 1)\n",
    "    depth_map *= 5.0\n",
    "    return depth_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb86522",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = depth_two_uint8_to_float(t[:,:,0], t[:, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01b8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t*100)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1993c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = torchvision.transforms.ToTensor()\n",
    "t_tensor = tf(t)\n",
    "print(t_tensor*100)\n",
    "print(t_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917b3a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as tf\n",
    "\n",
    "tf.crpo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5342608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'D:/datasets/STD/images/B1Random'\n",
    "for idx in range(320):\n",
    "    image_path = os.path.join(base_dir, 'SK_depth_%d.png' % idx)\n",
    "\n",
    "    io = imageio.imread(image_path)\n",
    "\n",
    "    depth = io[:,:,0]+io[:,:,1]*256\n",
    "    depth[depth < 200] = 0\n",
    "    depth[depth > 600] = 0\n",
    "\n",
    "    fig = plt.figure(1)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(depth)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab61e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(base_dir, 'SK_depth_321.png')\n",
    "\n",
    "io = imageio.imread(image_path)\n",
    "\n",
    "depth = io[:,:,0]+io[:,:,1]*256\n",
    "depth[depth < 200] = 0\n",
    "depth[depth > 600] = 0\n",
    "\n",
    "fig = plt.figure(1)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(depth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6e2540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvzone.HandTrackingModule import HandDetector\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95b6add",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('D:/datasets/RHD_v1-1/RHD_published_v2/training/color/00000.png')\n",
    "print(type(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5fb881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(detectionCon=0.8, maxHands=2)\n",
    "while True:\n",
    "    # Get image frame\n",
    "    # success, img = cap.read()\n",
    "    # print(type(success), type(img))\n",
    "    img = cv2.imread('D:/datasets/RHD_v1-1/RHD_published_v2/training/color/00000.png')\n",
    "    # Find the hand and its landmarks\n",
    "    hands, img = detector.findHands(img)  # with draw\n",
    "    # hands = detector.findHands(img, draw=False)  # without draw\n",
    "\n",
    "    if hands:\n",
    "        # Hand 1\n",
    "        hand1 = hands[0]\n",
    "        lmList1 = hand1[\"lmList\"]  # List of 21 Landmark points\n",
    "        bbox1 = hand1[\"bbox\"]  # Bounding box info x,y,w,h\n",
    "        centerPoint1 = hand1['center']  # center of the hand cx,cy\n",
    "        handType1 = hand1[\"type\"]  # Handtype Left or Right\n",
    "\n",
    "        fingers1 = detector.fingersUp(hand1)\n",
    "\n",
    "        if len(hands) == 2:\n",
    "            # Hand 2\n",
    "            hand2 = hands[1]\n",
    "            lmList2 = hand2[\"lmList\"]  # List of 21 Landmark points\n",
    "            bbox2 = hand2[\"bbox\"]  # Bounding box info x,y,w,h\n",
    "            centerPoint2 = hand2['center']  # center of the hand cx,cy\n",
    "            handType2 = hand2[\"type\"]  # Hand Type \"Left\" or \"Right\"\n",
    "\n",
    "            fingers2 = detector.fingersUp(hand2)\n",
    "\n",
    "            # Find Distance between two Landmarks. Could be same hand or different hands\n",
    "            length, info, img = detector.findDistance(lmList1[8], lmList2[8], img)  # with draw\n",
    "            # length, info = detector.findDistance(lmList1[8], lmList2[8])  # with draw\n",
    "    # Display\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    cv2.waitKey(1)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c592121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/datasets/cvpr14_MSRAHandTrackingDB/cvpr14_MSRAHandTrackingDB/Subject1/joint.txt'\n",
    "with open(path, 'r') as f:\n",
    "    listl = []\n",
    "    for line in f:\n",
    "        strip_lines = line.strip()\n",
    "        listli=strip_lines.split()\n",
    "        m = listl.append(np.asarray(listli))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9becc8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UVRLab\\AppData\\Local\\Temp/ipykernel_23172/1478361731.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  li = np.asarray(listl)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "li = np.asarray(listl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "017b0c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "li = np.loadtxt(path, dtype = float,delimiter=' ', skiprows = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecac4efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "li_reshape = li[0].reshape([-1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4132ec36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 3)\n"
     ]
    }
   ],
   "source": [
    "print(li_reshape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "972f4cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "jointlists = np.empty([0,21,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b7ba3525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 21, 3)\n",
      "(400, 21, 3)\n",
      "(400, 21, 3)\n",
      "(400, 21, 3)\n",
      "(400, 21, 3)\n",
      "(400, 21, 3)\n",
      "(2400, 21, 3)\n"
     ]
    }
   ],
   "source": [
    "path = 'D:/datasets/cvpr14_MSRAHandTrackingDB/cvpr14_MSRAHandTrackingDB'\n",
    "jointlists = np.empty([0,21,3])\n",
    "for i in range(1, 7):\n",
    "    joint_path = os.path.join(path, 'Subject%d' % i, 'joint.txt')\n",
    "    jointlist = np.loadtxt(joint_path, dtype=float, delimiter=' ', skiprows=1)\n",
    "    jointlist = [joint.reshape([-1,3]) for joint in jointlist] #[400,21,3]\n",
    "    print(np.asarray(jointlist).shape)\n",
    "    jointlists=np.append(jointlists, jointlist, axis=0)\n",
    "\n",
    "print(jointlists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc692c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "from utils.train_utils import orthographic_proj_withz as proj\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
