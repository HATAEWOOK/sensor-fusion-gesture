{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36980cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad48504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './ckp/results/E200_ 20_pred.png'\n",
    "path2 = './ckp/results/E200_ 20_target.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f850a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "io = imageio.imread(path)\n",
    "io2 = imageio.imread(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e110104c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 600, 4)\n"
     ]
    }
   ],
   "source": [
    "print(io.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f3a5f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOPUlEQVR4nO3db2hd933H8ffHkuOWJhB7vjLGsmcXRJhT2qQIL5AxsqSrvbbUJmBQoUMPDH7iQsoGxV5how8M2R6UPsoD04YJ2tQI2mARylajNpSREUdunMZ/4lqNs1jIWEpC03YP3Nj57sH9hZ3ZsnXte4/OZd/PC8Q596dz7/nKf94+9+omUkRgZnmtanoAM2uWI2CWnCNglpwjYJacI2CWnCNgllxtEZC0S9J5SbOSDtZ1HjPrjup4n4CkAeDXwF8Dc8ArwFci4mzPT2ZmXanrSmAHMBsRb0bEH4GjwO6azmVmXRis6XE3AZcqt+eAP7/VwevXr4+tW7fWNIqZAZw8efKdiGjduF5XBLTE2v953iFpP7AfYMuWLczMzNQ0ipkBSPqvpdbrejowB2yu3B4G5qsHRMSRiBiNiNFW66Y4mdkKqSsCrwAjkrZJugcYA6ZqOpeZdaGWpwMRcU3S14B/BwaAZyPiTB3nMrPu1PWaABHxE+AndT2+mfWG3zFolpwjYJacI2CWnCNglpwjYJacI2CWnCNglpwjYJacI2CWnCNglpwjYJacI2CWnCNglpwjYJacI2CWnCNglpwjYJacI2CWnCNgllxt/4/BfhURfPDBB02PYbYkSQwODiIt9aM76pEuAu+88w5PPvkk77//ftOjmN3kgQce4LnnnmP16tUrds50Ebh27Rpnz57lvffea3oUs5usWrWKOn5I8G3PuaJnM7O+4wiYJecImCXnCJgl5wiYJbdsBCQ9K2lB0unK2jpJxyVdKNu1lc8dkjQr6byknXUNbma90cmVwL8Cu25YOwhMR8QIMF1uI2k77R9D/mC5zzOSBno2rZn13LIRiIhfADd+U303MFH2J4A9lfWjEXE1Ii4Cs8CO3oxqZnW429cENkTEZYCyHSrrm4BLlePmypqZ9alevzC41Buel3z7k6T9kmYkzSwuLvZ4DDPr1N1G4IqkjQBlu1DW54DNleOGgfmlHiAijkTEaESMtlqtuxzDzLp1txGYAsbL/jhwrLI+JmmNpG3ACHCiuxHNrE7L/gdEkn4IPAaslzQH/BPwNDApaR/wNrAXICLOSJoEzgLXgAMRcb2m2c2sB5aNQER85RafeuIWxx8GDnczlJmtHL9j0Cw5R8AsOUfALDlHwCw5R8AsOUfALDlHwCw5R8AsOUfALDlHwCw5R8AsOUfALDlHwCw5R8AsOUfALDlHwCw5R8AsOUfALDlHwCw5R8AsOUfALDlHwCw5R8AsOUfALDlHwCw5R8AsOUfALDlHwCy5ZSMgabOkn0s6J+mMpKfK+jpJxyVdKNu1lfsckjQr6byknXV+AWbWnU6uBK4Bfx8RfwY8AhyQtB04CExHxAgwXW5TPjcGPAjsAp6RNFDH8GbWvWUjEBGXI+KXZf/3wDlgE7AbmCiHTQB7yv5u4GhEXI2Ii8AssKPHc5tZj9zRawKStgIPAy8DGyLiMrRDAQyVwzYBlyp3mytrNz7WfkkzkmYWFxfvYnQz64WOIyDpXuBHwNcj4ne3O3SJtbhpIeJIRIxGxGir1ep0DDPrsY4iIGk17QD8ICJ+XJavSNpYPr8RWCjrc8Dmyt2HgfnejGtmvdbJdwcEfA84FxHfrnxqChgv++PAscr6mKQ1krYBI8CJ3o1sZr002MExjwJ/C7wu6VRZ+wfgaWBS0j7gbWAvQESckTQJnKX9nYUDEXG914ObWW8sG4GI+A+Wfp4P8MQt7nMYONzFXGa2QvyOQbPkHAGz5BwBs+QcAbPkHAGz5BwBs+QcAbPkHAGz5BwBs+QcAbPkHAGz5BwBs+QcAbPkHAGz5BwBs+QcAbPkHAGz5BwBs+QcAbPkHAGz5BwBs+QcAbPkHAGz5BwBs+QcAbPkHAGz5BwBs+Q6+anEH5N0QtJrks5I+lZZXyfpuKQLZbu2cp9DkmYlnZe0s84vwMy608mVwFXg8Yj4DPAQsEvSI8BBYDoiRoDpchtJ24Ex4EFgF/CMpIEaZjezHlg2AtH2h3JzdfkIYDcwUdYngD1lfzdwNCKuRsRFYBbY0cuhzax3OnpNQNKApFPAAnA8Il4GNkTEZYCyHSqHbwIuVe4+V9ZufMz9kmYkzSwuLnbxJZhZNzqKQERcj4iHgGFgh6RP3eZwLfUQSzzmkYgYjYjRVqvV0bBm1nt39N2BiPgt8CLt5/pXJG0EKNuFctgcsLlyt2FgvttBzawenXx3oCXp/rL/ceBzwBvAFDBeDhsHjpX9KWBM0hpJ24AR4ESP5zazHhns4JiNwER5hX8VMBkRL0j6T2BS0j7gbWAvQESckTQJnAWuAQci4no945tZt5aNQET8Cnh4ifV3gSducZ/DwOGupzOz2vkdg2bJOQJmyTkCZsk5AmbJOQJmyTkCZsk5AmbJOQJmyTkCZsk5AmbJOQJmyTkCZsk5AmbJOQJmyTkCZsk5AmbJOQJmyTkCZsk5AmbJOQJmyTkCZsk5AmbJOQJmyTkCZsk5AmbJOQJmyTkCZsl1HAFJA5JelfRCub1O0nFJF8p2beXYQ5JmJZ2XtLOOwc2sN+7kSuAp4Fzl9kFgOiJGgOlyG0nbgTHgQWAX8Ez5icZm1oc6ioCkYeCLwHcry7uBibI/AeyprB+NiKsRcRGYBXb0ZFoz67lOrwS+A3wD+LCytiEiLgOU7VBZ3wRcqhw3V9bMrA8tGwFJXwIWIuJkh4+pJdZiicfdL2lG0szi4mKHD21mvdbJlcCjwJclvQUcBR6X9H3giqSNAGW7UI6fAzZX7j8MzN/4oBFxJCJGI2K01Wp18SWYWTeWjUBEHIqI4YjYSvsFv59FxFeBKWC8HDYOHCv7U8CYpDWStgEjwImeT25mPTHYxX2fBiYl7QPeBvYCRMQZSZPAWeAacCAirnc9qZnV4o4iEBEvAi+W/XeBJ25x3GHgcJezmdkK8DsGzZJzBMyScwTMknMEzJJzBMyScwTMknMEzJJzBMyScwTMknMEzJJzBMyScwTMknMEzJJzBMyScwTMknMEzJJzBMyScwTMknMEzJJzBMyScwTMknMEzJJzBMyScwTMknMEzJJzBMyScwTMkusoApLekvS6pFOSZsraOknHJV0o27WV4w9JmpV0XtLOuoY3s+7dyZXAX0XEQxExWm4fBKYjYgSYLreRtJ32jzB/ENgFPCNpoIczm1kPdfN0YDcwUfYngD2V9aMRcTUiLgKzwI4uzmNmNeo0AgH8VNJJSfvL2oaIuAxQtkNlfRNwqXLfubJmZn1osMPjHo2IeUlDwHFJb9zmWC2xFjcd1I7JfoAtW7Z0OIaZ9VpHVwIRMV+2C8DztC/vr0jaCFC2C+XwOWBz5e7DwPwSj3kkIkYjYrTVat39V2BmXVk2ApI+Iem+j/aBzwOngSlgvBw2Dhwr+1PAmKQ1krYBI8CJXg9uZr3RydOBDcDzkj46/rmI+DdJrwCTkvYBbwN7ASLijKRJ4CxwDTgQEddrmd7MurZsBCLiTeAzS6y/Czxxi/scBg53PZ2Z1c7vGDRLzhEwS84RMEvOETBLzhEwS84RMEuu07cN/7+xatUq1q5dS3nfg1lfuf/++1f8z2a6CKxfv56XXnqJDz/8sOlRzG4yODjI4ODK/rVMF4GBgQGGhoaWP9AsCb8mYJacI2CWnCNglpwjYJacI2CWnCNglpwjYJacI2CWnCNglpwjYJacI2CWnCNglpwjYJacI2CWnCNglpwjYJacI2CWnCNglpwjYJacI2CWnCNglpwjYJacIqLpGZC0CPw38E7Ts1Ssx/PcjudZXr/N9KcR0bpxsS8iACBpJiJGm57jI57n9jzP8vpxpqX46YBZco6AWXL9FIEjTQ9wA89ze55nef0400365jUBM2tGP10JmFkDGo+ApF2SzkualXRwhc75rKQFSacra+skHZd0oWzXVj53qMx3XtLOGubZLOnnks5JOiPpqSZnkvQxSSckvVbm+VaT81TOMSDpVUkv9Mk8b0l6XdIpSTP9MNNdiYjGPoAB4DfAJ4F7gNeA7Stw3r8EPgucrqz9C3Cw7B8E/rnsby9zrQG2lXkHejzPRuCzZf8+4NflvI3MBAi4t+yvBl4GHmny16ic5++A54AXmv49K+d5C1h/w1qjM93NR9NXAjuA2Yh4MyL+CBwFdtd90oj4BfDeDcu7gYmyPwHsqawfjYirEXERmC1z93KeyxHxy7L/e+AcsKmpmaLtD+Xm6vIRTc0DIGkY+CLw3cpyY/PcRj/OdFtNR2ATcKlye66sNWFDRFyG9l9KYKisr+iMkrYCD9P+17exmcql9ylgATgeEY3OA3wH+AbwYWWt6d+zAH4q6aSk/X0y0x0bbPj8WmKt375dsWIzSroX+BHw9Yj4nbTUqVdmpoi4Djwk6X7geUmfus3htc4j6UvAQkSclPRYJ3epc56KRyNiXtIQcFzSG30w0x1r+kpgDthcuT0MzDc0yxVJGwHKdqGsr8iMklbTDsAPIuLH/TATQET8FngR2NXgPI8CX5b0Fu2njI9L+n6D8wAQEfNluwA8T/vyvvHfszvVdAReAUYkbZN0DzAGTDU0yxQwXvbHgWOV9TFJayRtA0aAE708sdr/5H8POBcR3256JkmtcgWApI8DnwPeaGqeiDgUEcMRsZX2n5GfRcRXm5oHQNInJN330T7weeB0kzPdtaZfmQS+QPvV8N8A31yhc/4QuAx8QLvQ+4A/AaaBC2W7rnL8N8t854G/qWGev6B9afgr4FT5+EJTMwGfBl4t85wG/rGsN/ZrVDnPY/zvdwea/D37JO1X+18Dznz0Z7cffo3u9MPvGDRLrumnA2bWMEfALDlHwCw5R8AsOUfALDlHwCw5R8AsOUfALLn/AQtnnVxrSEM5AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(io)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acddea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(io[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa643ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833709e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = cv2.cvtColor(cv, cv2.COLOR_BGR2RGB)\n",
    "print(t[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1061e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_two_uint8_to_float(top_bits, bottom_bits):\n",
    "    \"\"\" Converts a RGB-coded depth into float valued depth. \"\"\"\n",
    "    depth_map = (top_bits * 2**8 + bottom_bits).astype('float32')\n",
    "    depth_map /= float(2**16 - 1)\n",
    "    depth_map *= 5.0\n",
    "    return depth_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb86522",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = depth_two_uint8_to_float(t[:,:,0], t[:, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01b8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t*100)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1993c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = torchvision.transforms.ToTensor()\n",
    "t_tensor = tf(t)\n",
    "print(t_tensor*100)\n",
    "print(t_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917b3a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as tf\n",
    "\n",
    "tf.crpo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5342608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'D:/datasets/STD/images/B1Random'\n",
    "for idx in range(320):\n",
    "    image_path = os.path.join(base_dir, 'SK_depth_%d.png' % idx)\n",
    "\n",
    "    io = imageio.imread(image_path)\n",
    "\n",
    "    depth = io[:,:,0]+io[:,:,1]*256\n",
    "    depth[depth < 200] = 0\n",
    "    depth[depth > 600] = 0\n",
    "\n",
    "    fig = plt.figure(1)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(depth)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab61e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(base_dir, 'SK_depth_321.png')\n",
    "\n",
    "io = imageio.imread(image_path)\n",
    "\n",
    "depth = io[:,:,0]+io[:,:,1]*256\n",
    "depth[depth < 200] = 0\n",
    "depth[depth > 600] = 0\n",
    "\n",
    "fig = plt.figure(1)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(depth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6e2540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvzone.HandTrackingModule import HandDetector\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95b6add",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('D:/datasets/RHD_v1-1/RHD_published_v2/training/color/00000.png')\n",
    "print(type(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5fb881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(detectionCon=0.8, maxHands=2)\n",
    "while True:\n",
    "    # Get image frame\n",
    "    # success, img = cap.read()\n",
    "    # print(type(success), type(img))\n",
    "    img = cv2.imread('D:/datasets/RHD_v1-1/RHD_published_v2/training/color/00000.png')\n",
    "    # Find the hand and its landmarks\n",
    "    hands, img = detector.findHands(img)  # with draw\n",
    "    # hands = detector.findHands(img, draw=False)  # without draw\n",
    "\n",
    "    if hands:\n",
    "        # Hand 1\n",
    "        hand1 = hands[0]\n",
    "        lmList1 = hand1[\"lmList\"]  # List of 21 Landmark points\n",
    "        bbox1 = hand1[\"bbox\"]  # Bounding box info x,y,w,h\n",
    "        centerPoint1 = hand1['center']  # center of the hand cx,cy\n",
    "        handType1 = hand1[\"type\"]  # Handtype Left or Right\n",
    "\n",
    "        fingers1 = detector.fingersUp(hand1)\n",
    "\n",
    "        if len(hands) == 2:\n",
    "            # Hand 2\n",
    "            hand2 = hands[1]\n",
    "            lmList2 = hand2[\"lmList\"]  # List of 21 Landmark points\n",
    "            bbox2 = hand2[\"bbox\"]  # Bounding box info x,y,w,h\n",
    "            centerPoint2 = hand2['center']  # center of the hand cx,cy\n",
    "            handType2 = hand2[\"type\"]  # Hand Type \"Left\" or \"Right\"\n",
    "\n",
    "            fingers2 = detector.fingersUp(hand2)\n",
    "\n",
    "            # Find Distance between two Landmarks. Could be same hand or different hands\n",
    "            length, info, img = detector.findDistance(lmList1[8], lmList2[8], img)  # with draw\n",
    "            # length, info = detector.findDistance(lmList1[8], lmList2[8])  # with draw\n",
    "    # Display\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    cv2.waitKey(1)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c592121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/datasets/cvpr14_MSRAHandTrackingDB/cvpr14_MSRAHandTrackingDB/Subject1/joint.txt'\n",
    "with open(path, 'r') as f:\n",
    "    listl = []\n",
    "    for line in f:\n",
    "        strip_lines = line.strip()\n",
    "        listli=strip_lines.split()\n",
    "        m = listl.append(np.asarray(listli))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9becc8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UVRLab\\AppData\\Local\\Temp/ipykernel_23172/1478361731.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  li = np.asarray(listl)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "li = np.asarray(listl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "017b0c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "li = np.loadtxt(path, dtype = float,delimiter=' ', skiprows = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecac4efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "li_reshape = li[0].reshape([-1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4132ec36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 3)\n"
     ]
    }
   ],
   "source": [
    "print(li_reshape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "972f4cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "jointlists = np.empty([0,21,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b7ba3525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 21, 3)\n",
      "(400, 21, 3)\n",
      "(400, 21, 3)\n",
      "(400, 21, 3)\n",
      "(400, 21, 3)\n",
      "(400, 21, 3)\n",
      "(2400, 21, 3)\n"
     ]
    }
   ],
   "source": [
    "path = 'D:/datasets/cvpr14_MSRAHandTrackingDB/cvpr14_MSRAHandTrackingDB'\n",
    "jointlists = np.empty([0,21,3])\n",
    "for i in range(1, 7):\n",
    "    joint_path = os.path.join(path, 'Subject%d' % i, 'joint.txt')\n",
    "    jointlist = np.loadtxt(joint_path, dtype=float, delimiter=' ', skiprows=1)\n",
    "    jointlist = [joint.reshape([-1,3]) for joint in jointlist] #[400,21,3]\n",
    "    print(np.asarray(jointlist).shape)\n",
    "    jointlists=np.append(jointlists, jointlist, axis=0)\n",
    "\n",
    "print(jointlists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc692c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "from utils.train_utils import orthographic_proj_withz as proj\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
